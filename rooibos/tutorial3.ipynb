{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18944af0",
   "metadata": {},
   "source": [
    "# Tutorial 3: Classification using simple statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb437a6d-139a-412e-bf54-fd62a9ed76f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4539b7e-97d0-4c8b-af08-ce89322657c7",
   "metadata": {},
   "source": [
    "### Welcome to the next-to-last practical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830509e5",
   "metadata": {},
   "source": [
    "This tutorial will show you how to get the computer to distinguish data between classes. There are many methods to classify data, but these can be summarised into (1) basic statistical methods; (2) machine learning; and (3) deep learning.\n",
    "\n",
    "We will continue with the rooibos data. Given the small size of this dataset, this notebook will focus on developing basic statistical methods for classifying tea samples into fermented and nonfermented. We will also estimate the accuracy of classification methods.  When estimating accuracy, it is important also to determine how exact this estimate is--so we will also show how to compute the standard deviation of the accuracy estimate using the so-called jackknife. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a264e8-219b-4454-8d61-564f2dbc24c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1936d48",
   "metadata": {},
   "source": [
    "### Overview of the classification problem  in general\n",
    "\n",
    "The following picture demonstrates the simple idea behind computational classification of objects. The general procedure is:\n",
    "* Extract features from data;\n",
    "* Pass the features into a model;\n",
    "* The model outputs a determination of the sample's class.\n",
    "\n",
    "<img src=\"pics/cat.png\" width=\"700\" height=\"300\">\n",
    "\n",
    "In this tutorial we are classifying tea samples, and the  features we will be using are the TPC, TEAC, and FRAP measurements obtained from chemical assays.\n",
    "\n",
    "To make this work, the programmer must:\n",
    "* Determine informative features to be used;\n",
    "* Develop a model that can effectively make use of the features extracted;\n",
    "* Evaluate the accuracy of the overall system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3061e",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "\n",
    "Binary classification supposes that we are trying to distinguish between two classes, denoted as \"null\" and \"positive\". In this case when distinguishing there are two possible errors: false positive or false negative:\n",
    "  * **[False positive](https://www.statisticshowto.com/false-positive-definition-and-examples/)**: This is where you receive a positive result for a test, when you should have received a negative (null) result. For example, it occurs when a cancer screening test comes back positive, but you don’t have the disease.\n",
    "  * **[False negative](https://www.statisticshowto.com/false-positive-definition-and-examples/)**: This is where a negative (null) test result is wrong. For example, in the Justice System, a false negative occurs when a guilty suspect is found “not guilty” and allowed to walk free.\n",
    "  \n",
    "Based on these definitions, we may define:\n",
    "\n",
    "   * _False positive rate_: What proportion of positive identifications was actually correct?\n",
    "   * _False negative rate_: What proportion of actual positives was identified correctly?\n",
    "\n",
    "These two work against each other: decreasing the false positive rate tends to increase the false negative rate.\n",
    "\n",
    "\n",
    "* False positive rate is related to [specificity  and precision](https://developers.google.com/machine-learning/crash-course/classification/check-your-understanding-accuracy-precision-recall)\n",
    "* False negative rate is related to [recall and sensitivity](https://developers.google.com/machine-learning/crash-course/classification/check-your-understanding-accuracy-precision-recall)\n",
    "\n",
    "### Threshold\n",
    "\n",
    "Usually binary classification is based on a single quantity calculated from the features of the data sample. If this quantity exceeds a threshold (which is set by the programmer) then the sample is judged to be in one class, and if not the sample is put in the other class.\n",
    "\n",
    "The following [figure](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) shows 30 predictions made by an email classification model. Those to the right of the classification _threshold_ are classified as \"spam\", while those to the left are classified as \"not spam.\"\n",
    "\n",
    "<img src=\"pics/preVSrec.svg\" width=\"700\" height=\"200\">\n",
    "\n",
    "where the results can be summaried as follows:\n",
    "\n",
    "<img src=\"pics/table.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "\n",
    "From these we may calculate:\n",
    "\n",
    "* _Precision_ (specificity) measures the percentage of **emails flagged as spam** that were correctly classified—that is, the percentage of dots to the right of the threshold line that are green in Figure above\n",
    "<img src=\"pics/pre.png\" width=\"350\" height=\"150\">\n",
    "* _Recall_ (sensitivity)  measures the percentage of **actual spam emails** that were correctly classified—that is, the percentage of green dots that are to the right of the threshold line in Figure above\n",
    "<img src=\"pics/rec.png\" width=\"330\" height=\"160\">\n",
    "* _Accuracy_ gives the overall percentage of correct classifications, and can be calculated  as follows: <br><br>\n",
    "<img src=\"pics/acc.png\" width=\"550\" height=\"270\">\n",
    "\n",
    "Different thresholds will give different values for these three measures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641d86c-d2cc-4346-a848-890a384c075e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[**Exercise 1:**](https://developers.google.com/machine-learning/crash-course/classification/check-your-understanding-accuracy-precision-recall) Consider a classification model that separates email into two categories: \"spam\" or \"not spam.\" If you raise the classification threshold, what will happen to precision? What about recall?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972523b-94ae-4aba-b41d-dc1f44a08aa2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18492750",
   "metadata": {},
   "source": [
    "#### Method 1: Receiver operating characteristic [(ROC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63dcb7b",
   "metadata": {},
   "source": [
    "Receiver operating characteristic (ROC) curves are used to determine thresholds based on the trade-off between false positive error and false negative error. The ROC curve tells you what false positive rate to expect for a given false negative rate.\n",
    "\n",
    "Our example will use code the python packages matplotlib, numpy, and sklearn. We will also make use of some pre-written programs that are located in the `source` directory--these will be imported later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31111d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 1___\n",
    "\n",
    "# Python packages \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3bf71-9926-4c32-a38e-6c65310945ef",
   "metadata": {},
   "source": [
    "Recover data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28832283-e91f-4061-85d8-af541e9178e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 2___\n",
    "\n",
    "%store -r df_nf\n",
    "%store -r df_fer\n",
    "%store -r df\n",
    "\n",
    "print(\"Shape of data frame (rows, columns) is: \", df.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494d657",
   "metadata": {},
   "source": [
    "Now, let's look at the imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 3___\n",
    "\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ff83d-734d-41b3-8eb0-5b9ab8929fd4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fdd75b-5072-4ab4-93cd-032c0b4d878b",
   "metadata": {},
   "source": [
    "Now let's create arrays for the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe038eb5-3d17-493d-b40e-d83aa3f9319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 4___\n",
    "\n",
    "feat_H2O = ['TPC_H2O', 'TEAC_H2O',  'FRAP_H2O'] # define the input (features) for H20\n",
    "feat_MEOH = ['TPC_MEOH', 'TEAC_MEOH',  'FRAP_MEOH'] # define the input (features) for MeOH\n",
    "\n",
    "y = df[['type']] # define the output (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c596127-4813-40bd-8e6d-6408958ee655",
   "metadata": {},
   "source": [
    "We consider the case where we are trying to use individual features to distinguish between fermented and nonfermented.  Since we have three different features, for each feature we can set a classification threshold.  The question is:  of these three features, which will give better classification results? We may use the ROC curves associated with each feature to determine this.\n",
    "\n",
    "Let's call the `draw_roc` function (from the file `roc` in the `source` directory) to draw the three ROC curves using the arrays we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb53a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 5___\n",
    "\n",
    "# The following code draws the ROC curves\n",
    "from source.roc import draw_roc \n",
    "\n",
    "# ROC curves\n",
    "di_h2o =[True, True, True] # if one of the features direction is below the 45 degrees, then we make it False\n",
    "draw_roc (df, di_h2o, feat_H2O, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146701a",
   "metadata": {},
   "source": [
    "In many references, the $x$ and $y$ axes are labeled as \"False positive\" and \"True positive\", respectively. Each line in the above graph shows the true positive and false positive rates for classifiers based on a threshold applied to a single feature.  The $x$ and $y$ values depend on the threshold that is set (the thresholds themselves not shown in the graph--only the rates are shown).  For example, if we use the `TEAC-H2O` features and set the threshold  so that the false positive rate is 0.5 (i.e. the specificity is 1- 0.5 = 0.5), then the true positive rate (recall) is about 0.3. \n",
    "\n",
    "The dotted line corresponds to random guessing:  if you simply make random guesses, then the true and false positive rates will be equal.  A good classifier will have a large true positive rate and small false positive rate. In other words, you want your ROC cuve to lie *above* the dotted line.  However, we can see that the TEAC_H2O and FRAP_H2O lines are *below* the dotted line.  This means that these classifiers are *worse* than random guessing.  How to solve this problem? Easy--instead of saying above the the threshold is 'positive', assign it to 'negative' (and similarly with below the threshold).  We can do this in the code by specifying the option `False` in line 2 of the above code for TEAC_H2O and FRAP_H2O. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c01bba-b164-489f-a4ee-f0cfdd2cdd2a",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 2:** Copy and change the above code so that all ROC curves lie above the dotted line.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44cdd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ___ code here ____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c3e11",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 3:** Make another graph for the three features based on MeOH. Make sure that all curves are above the dotted line.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e90622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ___ code here ____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007b47c-458c-45ab-b4a9-321067ea48b4",
   "metadata": {},
   "source": [
    "The ROC curve does not actually tell you which threshold to use--it simply tells you what precision/recall tradeoffs are possible.  The user will have to decide which tradeoff is most suitable for his situation, and set the threshold accordingly. \n",
    "\n",
    "ROC curves are especially useful if you want to compare different classifiers. Classifiers with higher ROC curves are better than classifiers with lower curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77444f",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 3:** Of the six features shown above, which gives the best classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61d190-8904-4120-8bed-e9ecf1602284",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbe963",
   "metadata": {},
   "source": [
    "### Using a statistical baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599ef35-aa7f-42ca-8f9a-bd8fc173eace",
   "metadata": {},
   "source": [
    "Simpler is better. Before trying more advanced ML classification techniques, it is always good to use a simple method to set a baseline for comparison.  The simplest classification methods are based on elementary statistics.\n",
    "\n",
    "One standard statistical classification method uses the so-called *Mahalanobis distance*. Given a probability distribution, the Mahalanobis distance of a data point to that distribution measures the \"likelihood\" that that data point belongs to the distribution. So if we have two or more distributions and want to assign a data point to one of them, we choose the distribution that gives the smallest [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). One big advantage of Mahalanobis distance is that it can be used to classify data based multiple features (unlike ROC curves, which only work for single-feature classification).\n",
    "\n",
    "This classification is easy to program in Python. We will demonstrate on our classification of rooibos into fermented and nonfermented. Recall that our data is contained in the dataframe `df`, and the `type` column indicates nonfermented (0) or fermented (1).  We call this our `target output`, and record it in the variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2ac26-2446-4c98-8222-20fac0cf881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 6___\n",
    "\n",
    "y = df[['type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36f4f4-0143-4a35-9950-796d4b6ecebd",
   "metadata": {},
   "source": [
    "Then let us list the feature groups that we will use for classification. Each feature group will give us a different classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 7___\n",
    "\n",
    "features = [ ['TPC_H2O'], ['TEAC_H2O'],  ['FRAP_H2O'], ['TPC_H2O', 'TEAC_H2O']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118ec32-f13e-4644-a9d6-e999670ca21c",
   "metadata": {},
   "source": [
    "At this point, we need to split the data between training data and testing data. The training data is used to  estimate the distribution parameters of fermented and nonfermented. Then, the testing data is used to evaluate the performance of the resulting classifiers.\n",
    "\n",
    "There is no set rule for choosing the split between training and testing, although generally most of the data is used for training.  To see which split gives the most reliable classifier, we use <i>jackknife</i> which gives error bars for each estimator's accuracy.  Jackknife works by going through the training/testing process multiple times, each time leaving out one data sample.  This produces multiple accuracy estimates, which can be combined using the \"jackknife formula\" to obtain an estimate for the variance of the classifier's accuracy.  For more background on the jackknife method, see https://en.wikipedia.org/wiki/Jackknife_resampling. \n",
    "\n",
    "First we list the different splits that we will evaluate. Splits are specified according to the proportion of data that is used for testing, while the rest of the data is used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8346783-caec-487d-8a4f-fc7527b4ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 8___\n",
    "\n",
    "splits = [0.4, 0.2]# Each number gives the proportion of testing data for a particular split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d5544-b80b-44dc-923c-91fb10715838",
   "metadata": {},
   "source": [
    "It is convenient to store the results in a *dictionary*.  A dictionary is a Python data structure consisting of key-value pairs. Just like in a regular dictionary, stored values are referenced by keyword, instead of using an index as in lists or arrays. \n",
    "\n",
    "In our case, we will create a *nested dictionary*, in which keys are assigned subkeys to label specific information. In our case, the top-level keys indicate the features and the training/testing split, and subkeys indicate different performance results obtained for each estimator.  For more information on nested dictionaries, see: https://www.geeksforgeeks.org/python-nested-dictionary/\n",
    "\n",
    "First we define an empty dictionary to hold our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 9___\n",
    "\n",
    "base_dict = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd52ed-92c7-47e7-bbf9-64442300e7b1",
   "metadata": {},
   "source": [
    "The following code trains and evaluates 8 classifiers (4 feature sets with 2 splits per feature set).  The code produces the following outputs for each classifier:\n",
    "\n",
    "* `tot_acc`    : total accuracy;\n",
    "* `jack_train` : list of accuracies after deleting individual training samples \n",
    "* `jack_test`  : list of accuracies after deleting samples from the testing\n",
    "\n",
    "As described above, we are calculating multiple accuracies for purpose of calculating the standard deviation using the jackknife formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a37159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 10___\n",
    "\n",
    "# The following code computes accuracies for classifiers based on \"Mahalanobis distance\" \n",
    "# (https://en.wikipedia.org/wiki/Mahalanobis_distance)\n",
    "from source.baseline_acc import get_accuracy_base \n",
    "\n",
    "# Loop through different splits (use the same splits for all features)\n",
    "for s in splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size= s, random_state=1, stratify = y, shuffle = True)\n",
    "    \n",
    "    # Loop through features\n",
    "    for f in features:\n",
    "        # Define training and testing sets\n",
    "        xtr =  X_train[f]\n",
    "        xte =  X_test[f]\n",
    "        # Use `get_accuracy_base` to obtain results for given classifier \n",
    "        results = get_accuracy_base (xtr, y_train, xte, y_test, len(f)) \n",
    "        \n",
    "        # Create keys in order to reference results\n",
    "        key = str(s)+\",\"+str(f) # Creates a top-level key for each estimator\n",
    "        \n",
    "        # Index results according to sub-keys\n",
    "        base_dict[key] = {} # Contains all results for this top-level key\n",
    "        base_dict[key]['tot_acc'] = results[0]\n",
    "        base_dict[key]['jack_train'] = results[1]\n",
    "        base_dict[key]['jack_test'] = results[2]\n",
    "        # print(base_dict)\n",
    "        # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe628e",
   "metadata": {},
   "source": [
    "We store the dictionary to use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 11___\n",
    "\n",
    "%store base_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba0661",
   "metadata": {},
   "source": [
    "Let us display the nested key structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 12___\n",
    "\n",
    "print(\"Top level keys are:\\n\", base_dict.keys())\n",
    "key = list(base_dict.keys())[0]\n",
    "\n",
    "print(\"\\n Subkeys are:\\n\",base_dict[key].keys() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f528a11",
   "metadata": {},
   "source": [
    "Now let's use these results to obtain the classifier standard deviations using jackknive, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70bea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Cell no. 13___\n",
    "\n",
    "# The following code implements jackknife \n",
    "from source.calculate_jack import jack_SD \n",
    "\n",
    "acc_arr = [] \n",
    "sd_arr = [] \n",
    "\n",
    "\n",
    "for key in base_dict.keys():\n",
    "    \n",
    "    \n",
    "    acc_arr.append(base_dict[key][ 'tot_acc' ]) # append total accuarcy to an array\n",
    "    sd_train = jack_SD( np.zeros( len(base_dict[key][ 'jack_train' ]) ), base_dict[key][ 'jack_train' ])[0]\n",
    "    sd_test = jack_SD( np.zeros( len(base_dict[key][ 'jack_test' ]) ), base_dict[key][ 'jack_test' ])[0]\n",
    "    sd = np.sqrt( np.array((sd_train**2)) + np.array((sd_test**2)))\n",
    "    sd_arr.append(sd) # append sd to array of standard deviations\n",
    "\n",
    "    # (Optional) print accuracy and standard deviation for each classifier\n",
    "    print( \"Classifier name:\"+str(key) )\n",
    "    print( \"Estimated accuracy: \" + str(base_dict[key][ 'tot_acc' ] ))\n",
    "    print(\"Standard deviation:\"+str(sd))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6945c1",
   "metadata": {},
   "source": [
    "The printed results are hard to interpret. It's much better to visualize using a graph. The `matplotlib` package `errorbar` makes it easy to plot error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b1528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 14___\n",
    "\n",
    "plt.figure(figsize=(10 , 5))\n",
    "plt.title( \"Accuracies for different features with SD\", fontweight ='bold', fontsize =12)\n",
    "plt.xlabel(\"Features\", fontweight ='bold', fontsize =12)\n",
    "plt.xticks(rotation = 60) # Rotates X-Axis Ticks by 45-degrees\n",
    "plt.ylabel(\"Accuracy\", fontweight ='bold', fontsize =12)\n",
    "plt.errorbar( list(base_dict.keys()), acc_arr, sd_arr, fmt='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d780a5",
   "metadata": {},
   "source": [
    "**Exercise 4**:  Based on the above graph:\n",
    "* Which of the better split:  40% testing or 20% testing? Justify your answer.\n",
    "* Which feature set gives the most accurate classifier? Justify your answer.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b97c8",
   "metadata": {},
   "source": [
    "It is very important to understand the correct interpretation of the error bars. We do not know the actual accuracy of the classifiers: with more training and testing data, the accuracy estimate of any classifier will change. The error bars show the level of uncertainty in each classifier's accuracy.  In other words, if we do a more thorough test with more data, we expect to obtain an accuracy somewhere within the error bars. \n",
    "\n",
    "In particular, the above graph shows that the `0.4,['TPC_H20']` classifier is statistically worse than the `0.4,['TPC_H20','TEAC_H20']` classifier because their error bars do not overlap.  On the other hand, although it looks like `0.4,['TPC_H20','TEAC_H20']` is better than `0.4,['FRAP_H20']`, still we cannot conclude that the difference between the two classifiers is statistically significant, because of the large error bars on the accuracy estimate for  `0.4,['FRAP_H20']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee148c5e",
   "metadata": {},
   "source": [
    "---\n",
    "**Challenge**\n",
    " 1. Now, try 50%, and 30% with all 3 features [TP, FRAP, TEAC] on the water extract and plot against ['TPC_H20','TEAC_H20'] feature.\n",
    " 2. repeat all tests using methanol instead of water.\n",
    " 3. Make a comparison between water and methanol extracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692063bc-d01c-4e0e-a349-f4e8f6a2095a",
   "metadata": {},
   "source": [
    "### _The END of this notebook_\n",
    "_see you in the next and the last tutorial_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b18b2-8cc8-43b6-a58e-3193d0e09541",
   "metadata": {},
   "source": [
    "<img src=\"pics/bam.png\" width=\"300\" height=\"200\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rooibos_hack",
   "language": "python",
   "name": "rooibos_hack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
