{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5688b4e8",
   "metadata": {},
   "source": [
    "# Tutorial 4: Classification using machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd0a18",
   "metadata": {},
   "source": [
    "#### Hello, and welcome to the final tutorial in this series!\n",
    "\n",
    "This tutorial will show you how to:\n",
    " - Use different machine learning methods (logistic regression and random forest) tools to classify rooibos teas\n",
    " - Apply parameter optimization to improve the classifiers\n",
    " - Compare those methods against each other using jackknife and a baseline\n",
    " \n",
    " Can't wait to get started! First we download the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a00d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python packages \n",
    "from sklearn.linear_model import LogisticRegression # a ML method\n",
    "from sklearn.ensemble import RandomForestClassifier # a ML method\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from source.calculate_jack import jack_SD # importing the baseline code from source.basline file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23645e6-3ef7-4ed0-b06a-3167688c416a",
   "metadata": {},
   "source": [
    "We also need the data stored in files `df` and `base_dict` which were created in Tutorial 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4635a-5b15-4533-934e-40f031480085",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df\n",
    "%store -r base_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba4424",
   "metadata": {},
   "source": [
    "### Set up machine learning methods and their hyper-parameters\n",
    "\n",
    "Logistic regression and random forest are leading machine learning (ML) methods. We will apply both and compare the reuslts. First we create aliases for the two classifiers, and create a list to use when we apply the classifiers one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4be85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression() # defining the model\n",
    "rf = RandomForestClassifier() # defining the model\n",
    "\n",
    "## Define a list of models, We will use this when we loop through ML methods.\n",
    "models = [ [lr, 'lr'], [rf, 'rf']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8750d",
   "metadata": {},
   "source": [
    "ML methods have *hyper-parameters* which must be defined before they can run. The hyper-parameter values used can significantly affect the performance of the methods. For this reason, *hyper-parameter optimization* is used to find the best possible values for these hyper-parameters. \n",
    "Basically, this is done using trial and error: each ML algorithm is trained with multiple combinations of hyper-parameters, and the best combination is chosen (this is called *grid search*).\n",
    " \n",
    "Each method has its own set of hyper-parameters. In the following code block we set possible values for three hyperparameters for logistic regression(LR), and three for random forest (RF). For more information on the meanings of these hyper-parameters, as well as a more complete description of grid search check the following links:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html (LR)\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html (RF)\n",
    "- https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/ (grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## logistic regression (LR)\n",
    "solvers = ['newton-cg', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [1000, 100, 10, 1.0, 0.1, 0.01, 0.001]\n",
    "lr_par = dict(solver=solvers,penalty=penalty,C=c_values) # creating a dictionary that contains the hyper-paramters\n",
    "\n",
    "## random forest (RF)\n",
    "n_estimators = range(10, 100, 10)\n",
    "rf_par = dict(random_state=[1], n_jobs=[-1], n_estimators=n_estimators) # creating a dictionary that contains the hyper-paramters\n",
    "\n",
    "## Define a list of parameter sets to go along with the list of models.\n",
    "parameters = [lr_par, rf_par]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d8203",
   "metadata": {},
   "source": [
    "To set up the training and testing sets, we need to specify inputs (feature sets), outputs (targets), and the training/testing split. We will apply each ML method to four different feature sets, and compare the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37732fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['type']] # define the targets\n",
    "features = [ ['TPC_H2O'], ['TEAC_H2O'],  ['FRAP_H2O'], ['TPC_H2O', 'TEAC_H2O']] # define a list of features\n",
    "splits = 0.4 # define a split (we will use 0.4, since it was the best in the prevoius notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823480a5-2cdc-49ad-acb4-918d4c6b837b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a8d6a-a063-46f5-999a-0b6f1ae06c14",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Just like the prevoius tutorial, the following code trains and evaluates 8 classifiers (4 feature sets with 2 ML models). The code produces the following outputs for each classifier:\n",
    "\n",
    "* `tot_acc`    : total accuracy;\n",
    "* `jack_train` : list of accuracies after deleting individual training samples \n",
    "* `jack_test`  : list of accuracies after deleting samples from the testing\n",
    "\n",
    "The `jack_train` and `jack_test` outputs are used to calculate the standard deviation of the accuracy using the jackknife formula. First we import the file used to produce these outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a175f-e598-4019-9762-442de9343c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.ml_acc import get_accuracy_ml #importing the ml_accuracy code from source ml_acc file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0d1ea-975f-4244-b343-2142d693591a",
   "metadata": {},
   "source": [
    "Next we define an empty dictionary to hold our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13c6dc-2c57-4396-b7f1-59abf64dd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dicts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47954618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different ML models and hyper parameters combinations (use the same splits for all features)\n",
    "\n",
    "for m, par in zip(models, parameters):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size= splits, random_state=1, stratify = y, shuffle = True)\n",
    "    \n",
    "    # defining The main subkeys, which are the machine learning models\n",
    "    key0 = str(m[1])\n",
    "    print(\"Main key for the following iterations:\")\n",
    "    print(key0)\n",
    "    \n",
    "    # Define sub-dictionaries for each main key to hold results for different feature combinations\n",
    "    ml_dicts[key0] = {} \n",
    "    \n",
    "    # Loop through features\n",
    "    for f in features:\n",
    "        xtr =  X_train[f] \n",
    "        xte =  X_test[f]\n",
    "        # Use `get_accuracy_ml` to obtain results for given classifier \n",
    "        results = get_accuracy_ml (m[0], par, np.array(xtr), np.array(y_train).flatten(), np.array(xte), np.array(y_test).flatten()) # to get the accuracies for the ml model\n",
    "        \n",
    "        key = str(splits)+\",\"+str((f)) # Create keys for the each feature set in order to reference results\n",
    "        # Create subdictionary for each feature combinations to hold results for that specific combo. \n",
    "        ml_dicts[key0][key] = {}\n",
    "\n",
    "        # Put results into dictionary\n",
    "        ml_dicts[key0][key]['tot_acc'] = results[0]\n",
    "        ml_dicts[key0][key]['jack_train'] = results[1]\n",
    "        ml_dicts[key0][key]['jack_test'] = results[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfd63d",
   "metadata": {},
   "source": [
    "Let's take a look at how the outputs are organized. We'll print some of the dictionary keys in the output data structure, which is called 'ml_dicts'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The main keys are: \"+ str(ml_dicts.keys()) + \" which stands for the main ML models\\n\")\n",
    "print(\"The main subkeys for linear regression are:\")\n",
    "print(str(ml_dicts['lr'].keys()) + \" which stands for the main ML features\\n\")\n",
    "print(\"The subkeys inside the feature set 'TPC_H2O' are:\")\n",
    "print(str(ml_dicts['lr'][\"0.4,['TPC_H2O']\"].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64128103",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 1:** Display the same results for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a237bc-8a6f-4a6a-be7a-b46f0f9561fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ___ code here ____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaeef0f-92d3-43db-b3ff-34f780074d8b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcb5d3",
   "metadata": {},
   "source": [
    "#### Using jackknife to evaluate the accuracy of ML methods\n",
    "\n",
    "As in Tutorial 3, we use jackknife to estimate the standard deviations of the accuracies for the different methods. Since we have run the models multiple times on each feature set, we can use the jackknife formula to estimate the standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12552e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_all = []\n",
    "for m, d in zip (models, ml_dicts.keys()):\n",
    "    acc_arr = [] \n",
    "    sd_arr = [] \n",
    "\n",
    "    # print(ml_dicts[d])\n",
    "    for key in ml_dicts[d].keys():\n",
    "        acc_arr.append(ml_dicts[d][key][ 'tot_acc' ]) # append total accuracy to an array\n",
    "        sd_train = jack_SD(np.zeros( len(ml_dicts[d][key][ 'jack_train' ]) ), ml_dicts[d][key][ 'jack_train' ])[0]\n",
    "        sd_test = jack_SD(np.zeros( len(ml_dicts[d][key][ 'jack_test' ]) ), ml_dicts[d][key][ 'jack_test' ])[0]\n",
    "        sd = np.sqrt( np.array((sd_train**2)) + np.array((sd_test**2)))\n",
    "        sd_arr.append(sd) # append the SD to the sd_arr\n",
    "    arr_all.append([ list(ml_dicts[d].keys()), acc_arr, sd_arr])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632aa9a7",
   "metadata": {},
   "source": [
    "Just to double check we're OK, we print the results. (We will graph them in a minute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0682d53-ef78-44f3-8e2b-d00ebd181ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr_all) # should we delete this line?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73744243-a407-448d-a79e-5ff9d3afb2ef",
   "metadata": {},
   "source": [
    "#### Graphing the results\n",
    "By graphing the results and error bars for each feature set and method, we make it easy to compare accuracies across the different feature sets and ML methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217374ce-fd4b-4740-a7b5-e3dbac8ff3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'purple', 'green', 'orange', 'red', 'brown']\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title( \"Accuracy  for different features with the SD\", fontweight ='bold', fontsize =12)\n",
    "plt.xlabel(\"Features\", fontweight ='bold', fontsize =12)\n",
    "plt.ylabel(\"Accuracy\", fontweight ='bold', fontsize =12)\n",
    "\n",
    "count = 0\n",
    "n = 5\n",
    "\n",
    "space = []\n",
    "tickFeat = []\n",
    "\n",
    "for result, model, color in zip(arr_all, models, colors):\n",
    "    a = np.linspace(n*count, n*(1+count)-2,len(features)) # to get index on the x-axis\n",
    "    space.extend(a)\n",
    "    tickFeat.extend(result[0])\n",
    "    plt.errorbar( a, result[1], result[2], fmt='o', label =model[1], color = color)\n",
    "    count += 1\n",
    "\n",
    "plt.xticks(space, tickFeat, rotation = 'vertical',  fontsize =12)\n",
    "plt.ylim(.2, 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22295e",
   "metadata": {},
   "source": [
    "From the graph, it appears that LR possibly has a slight advantage over RF.  The accuracies seem to be slightly higher, and the error bars a bit smaller. However, none of the differences are statistically significant, because error bars overlap.\n",
    "The only statistically significant result from the experiment is that the LR classifier based on two features is more accurate than those based on TPC_H20 and FRAP_H2O for LR and TPC_H2O for RF.  We can draw this conclusion because the error bars do not overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5cf9a",
   "metadata": {},
   "source": [
    "### Displaying performance relative to the baseline\n",
    "\n",
    "Do these classifiers really give us an advantage over a very simple statistical classifier that is based on Mahalanobis distance? We choose as our baseline the best Mahalanobis classifier that we found in the previous tutorial, and we plot the accuracy difference between our ML classifers and this baseline (with error bars).\n",
    "\n",
    "First we create a list of relative accuracies and standard deviations for the different ML classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acba26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List containing data for different classifiers\n",
    "arr_diff_all = []\n",
    "\n",
    "# Loop through models to complie all data\n",
    "for m, m_key in zip (models, ml_dicts.keys()):\n",
    "    acc_diff_arr = [] \n",
    "    sd_diff_arr = [] \n",
    "    for f_key in ml_dicts[m_key].keys():\n",
    "        \n",
    "        \n",
    "        acc_diff_arr.append( ml_dicts[m_key][f_key][ 'tot_acc' ] -  base_dict[\"0.4,['TPC_H2O', 'TEAC_H2O']\"][ 'tot_acc' ]  )\n",
    "\n",
    "        sd_train = jack_SD( base_dict[\"0.4,['TPC_H2O', 'TEAC_H2O']\"][ 'jack_train' ],  ml_dicts[m_key][f_key]['jack_train'] )[0]\n",
    "        sd_test =  jack_SD(  base_dict[\"0.4,['TPC_H2O', 'TEAC_H2O']\"][ 'jack_test' ] ,  ml_dicts[m_key][f_key]['jack_test']   )[0]\n",
    "\n",
    "        sd = np.sqrt( np.array((sd_train**2)) + np.array((sd_test**2)))\n",
    "        \n",
    "        sd_diff_arr.append(sd) # append sd_arr to an array\n",
    "    arr_diff_all.append([ list(ml_dicts[m_key].keys()), acc_diff_arr, sd_diff_arr])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1855b66",
   "metadata": {},
   "source": [
    "The information is stored in a list that tells the train/test split and features; mean accuracy difference; and standard deviation of accuracy for the baseline and ML classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr_diff_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd01c2-4191-42b9-8a29-6da6e05a7057",
   "metadata": {},
   "source": [
    "Since the results are arranged in an orderly fashion, we can now graph them. Since we are measuring relative to the baseline, the baseline appears as a solid black line at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe3f69-e5a1-4d05-a193-caefb098e02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# colors = ['purple', 'green', 'orange', 'red', 'brown']\n",
    "colors = ['purple', 'green'] # colors for the different methods\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title( \"Accuracy  difference vs. baseline for ML methods\", fontweight ='bold', fontsize =12)\n",
    "plt.xlabel(\"Features\", fontweight ='bold', fontsize =12)\n",
    "plt.ylabel(\"Accuracy  difference from baseline\", fontweight ='bold', fontsize =12)\n",
    "count = 0\n",
    "n = 5\n",
    "space = [] # will be used to label the x-axis\n",
    "tickFeat = [] # will be used to label the x-axis\n",
    "\n",
    "for result, model, color in zip(arr_diff_all, models, colors):\n",
    "    a = np.linspace(n*count, n*(1+count)-2,len(features))\n",
    "    space.extend(a)\n",
    "    tickFeat.extend(result[0])\n",
    "    plt.errorbar( a, result[1], result[2], fmt='o', label =model[1], color = color)\n",
    "    count += 1\n",
    "    \n",
    "plt.plot(np.array(space), np.zeros(len(features)*len(models)), color = 'Black')        \n",
    "plt.xticks(space, tickFeat, rotation = 'vertical',  fontsize =12)\n",
    "plt.ylim(-.5, .5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ce854-ff96-4707-a51a-72edfadb707c",
   "metadata": {},
   "source": [
    "The graph shows that in this case, neither LR no RF gives any statistically significant advantage over the baseline method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca18c3-3e33-42f4-9c99-e6fd13893124",
   "metadata": {},
   "source": [
    "### Now it's your turn\n",
    " 1. Try all 3 features [TP, FRAP, TEAC] on water extracts for both LR and RF.\n",
    " 2. Try other ML methods! See if you can find one that performs better (I bet you can't!)\n",
    " 2. Repeat all tests using methanol instead of water.\n",
    " 3. Make a comparison between water and methanol extracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66bf93-d309-458e-a4a9-ae4f9668ee7e",
   "metadata": {},
   "source": [
    "### _The END_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa36d82-6f92-4d1c-9a3e-4898fadb35af",
   "metadata": {},
   "source": [
    "<img src=\"pics/hap.jpg\" width=\"300\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac48610-ac2f-4c81-8726-27a7acde0240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rooibos_hack",
   "language": "python",
   "name": "rooibos_hack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
